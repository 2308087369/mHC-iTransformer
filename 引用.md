好像引用都放上来，评论区附上：
DLinear - Nixtla - Nixtlaverse
Architecture of PatchTST [15]. - ResearchGate
Influential Time-Series Forecasting Papers of 2023-2024: Part 1 | Towards Data Science
thuml/Time-Series-Library: A Library for Advanced Deep Time Series Models for General Time Series Analysis. - GitHub
DeformableTST: Transformer for Time Series Forecasting without Over-reliance on Patching - NIPS papers
TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting - arXiv
Official implementation for "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting" (ICLR 2024 Spotlight) - GitHub
TimeMachine: A Time Series is Worth 4 Mambas for Long-Term Forecasting - PMC - NIH
TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting - GitHub
TimeMachine: A Time Series is Worth 4 Mambas for Long-Term Forecasting - ResearchGate
MambaTS: Improved Selective State Space Models for Long-term ...
Time-LLM: Time Series Forecasting by Reprogramming Large Language Models - arXiv
TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS - ICLR Proceedings
Chronos: Learning the Language of Time Series - UC Berkeley ...
Chronos-T5 - QuantConnect.com
Chronos: Learning the Language of Time Series - arXiv
Lag-Llama: An Open-Source Base Model for Predicting Time Series Data - Medium
Paper Review: Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting - Andrey Lukyanenko
time-series-foundation-models/lag-llama: Lag-Llama ... - GitHub
Empowering Time Series Analysis with Foundation Models: A Comprehensive Survey
MOIRAI: Salesforce's Foundation Model for Time-Series Forecasting
Unified Training of Universal Time Series Forecasting Transformers - arXiv
Time series foundation models can be few-shot learners - Google Research
TimesFM-ICF - Dejan.ai
Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts
Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts
Time-MoE Billion-Scale Time Series Foundation Models With Mixture of Experts - Scribd
Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts
Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts - GitHub
SalesforceAIResearch/gift-eval - GitHub
GIFT Eval - a Hugging Face Space by Salesforce
First-Place Results on the GIFT-Eval Benchmark - - timecopilot.dev